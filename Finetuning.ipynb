{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAcN6BFvVmOAZvmR+daR/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nafiseh-Izadyar/CSC581B-/blob/main/Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMQLG8G2drmw"
      },
      "source": [
        "### **Introduction to Deep Learning for Image Classification - CSC 581B - A01**\n",
        "*Nafiseh Izadyar*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAJFy-44dgaK"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLYOVnHQd3wT"
      },
      "source": [
        "Loading the CIFAR-100 dataset and splitting into train, val, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nPAs6Pbd1UW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import time\n",
        "\n",
        "# Loading dataset\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "      transforms.RandomHorizontalFlip(0.5),\n",
        "      transforms.RandomVerticalFlip(0.5),\n",
        "      transforms.RandomRotation(15),\n",
        "      transforms.RandomCrop(32,4),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "      transforms.Scale((224,224))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "      transforms.Resize(32),\n",
        "      transforms.CenterCrop(32),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "      transforms.Scale((224,224))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# loading train and Validation Set\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "print('Train Size:', len(trainset))\n",
        "\n",
        "# Validation Set\n",
        "torch.manual_seed(32)\n",
        "validation_size = 1000\n",
        "train_size = len(trainset) - validation_size\n",
        "\n",
        "\n",
        "\n",
        "train, validation = torch.utils.data.random_split(trainset,[train_size,validation_size]) \n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
        "validationloader = torch.utils.data.DataLoader(validation, batch_size = batch_size, num_workers = 4)\n",
        "\n",
        "# loading test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "print('Test Size: ',len(testset))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers = 4)\n",
        "\n",
        "\n",
        "print('Classes: ')\n",
        "print(trainset.classes)\n",
        "\n",
        "print('Done!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMKFBwreAmQ"
      },
      "source": [
        "Checking if the GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dizAbWNAeGX-"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU is available')\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print('No GPU')\n",
        "  device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThY1wND0eMBE"
      },
      "source": [
        "Defining accuracy, plot, and testing functions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdcCJe30eKcz"
      },
      "source": [
        "def accuracy(output,label):\n",
        "  _, preds = torch.max(output, dim=1)\n",
        "  return torch.tensor(torch.sum(preds == label).item() / len(preds))\n",
        "\n",
        "def ploting(printloss,validation_loss,printval,printtrain,learning_rate):\n",
        "  # plot loss\n",
        "  plt.plot(printloss,'-o',label='train')\n",
        "  plt.plot(validation_loss,'-o',label='validation')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('loss')\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  # plott accuracy on validation set\n",
        "  plt.plot(printval,'-o',label = 'validation')\n",
        "  plt.plot(printtrain,'-o',label='train')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Accuracy of validation set in different epochs')\n",
        "  plt.legend(loc =\"upper left\")\n",
        "  plt.figure()\n",
        "\n",
        "  # plot learning rate\n",
        "  plt.plot(learning_rate,'-o')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Learning rate')\n",
        "  plt.title('Learning Rate Decay')\n",
        "\n",
        "def testModel(model,test):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "  print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erpbWljZeREx"
      },
      "source": [
        "Pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHquJywkeURe"
      },
      "source": [
        "import warnings\n",
        "def select_model(m):\n",
        "  if m == 'alexnet':\n",
        "    # AlexNet\n",
        "    model = models.alexnet(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(4096,100)\n",
        "    model.to(device)\n",
        "    return model\n",
        "  elif m == 'resnet18':\n",
        "    # Resnet18\n",
        "    model = models.resnet18(pretrained = True)\n",
        "    model.fc = nn.Linear(512, 100)\n",
        "    model.to(device)\n",
        "    return model\n",
        "  elif m == 'densenet':\n",
        "    # DenseNet\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    model.classifier = nn.Linear(1024, 100)\n",
        "    model.to(device)\n",
        "    warnings.warn('Out of Memory Error')\n",
        "    return model\n",
        "  elif m == 'googlenet':\n",
        "    #googlenet\n",
        "    model = models.googlenet(pretrained=True)\n",
        "    print(model)\n",
        "    model.fc = nn.Linear(1024,100)\n",
        "    model.to(device)\n",
        "    return model\n",
        "  elif m == 'inception':\n",
        "    # Inception\n",
        "    model = models.inception_v3(pretrained=True)\n",
        "    model.AuxLogits.fc = nn.Linear(768, 100)\n",
        "    model.fc = nn.Linear(2048,100)\n",
        "    model.to(device)\n",
        "    return model\n",
        "  elif m == 'squeezenet':\n",
        "    ##squeezenet\n",
        "    model = models.squeezenet1_0(pretrained=True)\n",
        "    model.classifier[1] = nn.Conv2d(512, 100, kernel_size=(1,1), stride=(1,1))\n",
        "    model.num_classes = 100\n",
        "    model.to(device)\n",
        "    return model\n",
        "  else:\n",
        "    assert('Model does not exist!')\n",
        "\n",
        "m = 'alexnet'\n",
        "model = select_model(m)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtgVkOcecEV"
      },
      "source": [
        "Defining the optimizer, criteria, and learning rate schedualer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqIa2gFTeh_r"
      },
      "source": [
        "# Defining the optimizer, criteria, and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1,weight_decay=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "schedualer = optim.lr_scheduler.MultiStepLR(optimizer,[5,20])\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52rxv0CekQx"
      },
      "source": [
        "The validation part has been commented becasue the GPU would ran out of memory, but works if enough memory is available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9fFK228e0jf"
      },
      "source": [
        "# Tracking the training time\n",
        "a = time.time()\n",
        "\n",
        "# tracking loss\n",
        "total_loss = []\n",
        "\n",
        "# tracking train and validation accuracy\n",
        "val_acc = []\n",
        "train_acc = []\n",
        "validation_loss = []\n",
        "\n",
        "# tracking the learning-rate\n",
        "learning_rate = []\n",
        "\n",
        "# train for number of epochs\n",
        "epochs = 25\n",
        "for iter in range(epochs):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader,0):\n",
        "    torch.cuda.empty_cache()\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    del data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # calculating the loss\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i % 313 == 312:\n",
        "      print('[%d, %5d] loss: %.3f' % (iter + 1, i + 1, running_loss / 313))\n",
        "      total_loss.append(running_loss / 313)\n",
        "      running_loss = 0.0\n",
        "\n",
        "  \n",
        "  learning_rate.append(lr)\n",
        "\n",
        "  # keeping track of train acc\n",
        "  acc_train = accuracy(outputs,labels)\n",
        "  train_acc.append(acc_train)\n",
        "  del inputs, labels\n",
        "\n",
        "  # Validation\n",
        "  val_loss = []\n",
        "\n",
        "  # for j, valdata in enumerate(validationloader,0):\n",
        "  #   valin, vallabel = valdata[0].to(device), valdata[1].to(device)\n",
        "  #   del valdata\n",
        "  #   valout = model(valin) \n",
        "  #   val_loss.append(criterion(valout,vallabel))\n",
        "  #   acc = accuracy(valout,vallabel)\n",
        "  #   del valin, vallabel, valout\n",
        "  \n",
        "  # epoch_loss = torch.stack(val_loss).mean()\n",
        "  # validation_loss.append(epoch_loss)\n",
        "  # print('[%d, %5d] val loss: %.3f' % (iter + 1, i + 1, epoch_loss))\n",
        "  # print('[%d, %5d] val accuracy: %.3f' % (iter + 1, i + 1, acc * 100))\n",
        "  # val_acc.append(acc)\n",
        "\n",
        "  schedualer.step()\n",
        "\n",
        "b = time.time()\n",
        "print(b-a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqoffkm1frfa"
      },
      "source": [
        "Testing and ploting the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MStvft7frKv"
      },
      "source": [
        "ploting(total_loss,validation_loss,val_acc,train_acc,learning_rate)\n",
        "testModel(model,testloader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}